[
  {
    "objectID": "experiments/exp_1_attention_accuracy/GOOGLE_SHEETS_SETUP.html",
    "href": "experiments/exp_1_attention_accuracy/GOOGLE_SHEETS_SETUP.html",
    "title": "Google Sheets Setup",
    "section": "",
    "text": "Both experiments share the same Google Sheets configuration.\nSee ../exp_2_memory/GOOGLE_SHEETS_SETUP.md for: - Complete Google Apps Script code - Sheet structure and column reference - Troubleshooting guide\n\n\nData from this experiment goes to the att_acc sheet with the following key columns:\n\n\n\nColumn\nDescription\n\n\n\n\nparticipant_id\nUnique ID\n\n\ntrial_number\n1-16 or “practice”\n\n\nindividual_id\nWhich face was shown\n\n\nsize_condition\n“big” or “small”\n\n\nsmile_condition\n“smile” or “nosmile”\n\n\nrace_response / race_correct\nRace question data\n\n\nsmile_response / smile_correct\nSmile question data"
  },
  {
    "objectID": "experiments/exp_1_attention_accuracy/GOOGLE_SHEETS_SETUP.html#quick-reference",
    "href": "experiments/exp_1_attention_accuracy/GOOGLE_SHEETS_SETUP.html#quick-reference",
    "title": "Google Sheets Setup",
    "section": "",
    "text": "Data from this experiment goes to the att_acc sheet with the following key columns:\n\n\n\nColumn\nDescription\n\n\n\n\nparticipant_id\nUnique ID\n\n\ntrial_number\n1-16 or “practice”\n\n\nindividual_id\nWhich face was shown\n\n\nsize_condition\n“big” or “small”\n\n\nsmile_condition\n“smile” or “nosmile”\n\n\nrace_response / race_correct\nRace question data\n\n\nsmile_response / smile_correct\nSmile question data"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "with Terry Gregory (LISER)\n\n\nA firm-level indicator of Artificial Intelligence adoption in Europe, covering more than three million firm websites from Belgium, France, Germany, and Luxembourg (2016–2024). The repository contains the data and documentation for the MAP-AI indicator.\n\n\nData Repository"
  },
  {
    "objectID": "data.html#datasets",
    "href": "data.html#datasets",
    "title": "Data",
    "section": "",
    "text": "with Terry Gregory (LISER)\n\n\nA firm-level indicator of Artificial Intelligence adoption in Europe, covering more than three million firm websites from Belgium, France, Germany, and Luxembourg (2016–2024). The repository contains the data and documentation for the MAP-AI indicator.\n\n\nData Repository"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "with Christina Gathmann (LISER) Labour Economics, Vol. 82, June 2023, 102343\n\n\nSeveral European countries have reformed their citizenship policies over the past decades. There is much to learn from their experience of how citizenship works; for whom it works; and what rules and policies matter for integration. The article surveys recent quasi-experimental evidence and field experiments from the social sciences on the link between eligibility rules, take-up and integration outcomes. Across countries and reforms, the evidence shows that faster access to citizenship increases take-up and improves the economic, educational, political and social integration of immigrants. Other eligibility rules like civic knowledge tests or application fees also impact who naturalizes and therefore benefits from citizenship. Birthright citizenship, which is much less common in Europe, turns out to be a powerful tool for getting second-generation immigrants off to a good start. Together, citizenship acts as a powerful catalyst benefiting immigrants as well as host countries.\n\n\nPublished Version Working Paper"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "",
    "text": "with Christina Gathmann (LISER) Labour Economics, Vol. 82, June 2023, 102343\n\n\nSeveral European countries have reformed their citizenship policies over the past decades. There is much to learn from their experience of how citizenship works; for whom it works; and what rules and policies matter for integration. The article surveys recent quasi-experimental evidence and field experiments from the social sciences on the link between eligibility rules, take-up and integration outcomes. Across countries and reforms, the evidence shows that faster access to citizenship increases take-up and improves the economic, educational, political and social integration of immigrants. Other eligibility rules like civic knowledge tests or application fees also impact who naturalizes and therefore benefits from citizenship. Birthright citizenship, which is much less common in Europe, turns out to be a powerful tool for getting second-generation immigrants off to a good start. Together, citizenship acts as a powerful catalyst benefiting immigrants as well as host countries.\n\n\nPublished Version Working Paper"
  },
  {
    "objectID": "research.html#working-papers-work-in-progress",
    "href": "research.html#working-papers-work-in-progress",
    "title": "Research",
    "section": "Working Papers / Work in Progress",
    "text": "Working Papers / Work in Progress\n\nArriving LATE: Access to Citizenship and Economic Integration\n\nwith Christina Gathmann (LISER)\n\n\nWe analyze whether faster access to citizenship fosters the economic integration of immigrants. Our empirical setting is Germany, which went from a strict concept of citizenship based on ‘jus sanguinis’ to a more open citizenship policy. We make use of discontinuities in residency requirements faced by first-generation immigrants to estimate LATEs based on Local Randomization and Fuzzy RDD approaches. We find that a more liberal citizenship policy acts as a catalyst for integration, especially for immigrant women. Women’s labor force participation increases by 8.9 percentage points and their earnings by 21.3%. We do not find any significant effects on immigrant men.\n\n\n\nSmall Pictures, Big Biases: The Adverse Effect of an Airbnb Design Intervention\n\nwith Carlotta Montorsi (LISER)\n\n\nA 2018 Airbnb design intervention reduced the size of the host profile picture, creating a natural experiment to test whether the salience of visual cues affects racial bias in the demand for Airbnb listings. Using scraped data from Airbnb in New York City and a face classification model, we find that, unexpectedly, the new design increased the Black-White demand disparity by 3.3 percentage points, an increase of about 30% relative to the pre-intervention gap. We show that smaller images made it harder for guests to detect positive facial cues – especially smiles – that are typically associated with higher demand, leading them to rely more heavily on skin color. In response, Black hosts updated their profile pictures to make their faces more visible and added basic amenities to their listings.\n\n\n\nThe Diffusion of Artificial Intelligence Across Firms: Evidence from Europe\n\nwith Terry Gregory (LISER)\n\n\nWe develop a novel firm-level indicator of Artificial Intelligence adoption in Europe (MAP-AI) by extracting information on AI usage from more than three million firm websites from Belgium, France, Germany, and Luxembourg (2016–2024) using a Large Language Model. The indicator captures realized AI adoption as signaled on their website rather than potential exposure. Our method allows to detect not only whether firms adopt AI, but also their role in the AI ecosystem and the type of AI technology they employ. Validation against human-coded benchmarks and external referenecs confirms high accuracy and external validity. We find that the share of AI-active firms grew from 1% in 2016 to 12% in 2024, with acceleration after 2022. We document a structural transformation in the AI ecosystem, as expanding AI adoption increases the share of adopters in overall AI activity, signaling widespread diffusion and more integrated AI use, including generative AI. While adoption is concentrated among larger, younger, knowledge-intensive firms in urban innovation clusters, workforce skills emerge as a key factor associated with AI adoption. Our skill-level analysis shows that foundational data skills form a necessary base for adoption, while a small set of specialized AI skills – such as machine learning and natural language processing – act as strong complements, highlighting human capital as a central driver of AI diffusion across firms.\n\n\nData Repository"
  },
  {
    "objectID": "posts/bert_topic_websites_lux/index.html",
    "href": "posts/bert_topic_websites_lux/index.html",
    "title": "What Does Luxembourg’s Web Talk About?",
    "section": "",
    "text": "In my previous post, I found that 75% of Luxembourg websites offer French, while Luxembourgish appears on just 9%. But language is only part of the story — what are these websites actually about?\nUsing the same CommonCrawl sample, I aggregated all crawled text per website-year, removed repeated boilerplate (navigation bars, footers, cookie banners), and let BERTopic discover themes on its own — no predefined categories, just multilingual embeddings, clustering, and keyword extraction across all years at once."
  },
  {
    "objectID": "posts/bert_topic_websites_lux/index.html#a-map-of-luxembourgs-web",
    "href": "posts/bert_topic_websites_lux/index.html#a-map-of-luxembourgs-web",
    "title": "What Does Luxembourg’s Web Talk About?",
    "section": "A Map of Luxembourg’s Web",
    "text": "A Map of Luxembourg’s Web\nWhat happens when you let an algorithm read every .lu website and group them by content? The treemap below shows the topics discovered in 2024, organized into 15 categories. Click any category to explore its individual topics, then click the header to zoom back out.\n\n\nAll topics discovered in 2024, grouped by category\n\n\n\nShow code\n# Compute category totals\ncat_totals = {}\nfor t in topics_2024:\n    s = t['sector']\n    cat_totals[s] = cat_totals.get(s, 0) + t['count']\n\nsorted_cats = sorted(cat_totals.keys(), key=lambda s: -cat_totals[s])\ntotal_classified = sum(cat_totals.values())\n\n# Build treemap arrays\nids = ['Luxembourg .lu']\nlabels = ['Luxembourg .lu']\nparents = ['']\nvalues = [total_classified]\ncolors = ['#faf9f7']\ncustomdata = [f'{total_classified:,} websites classified']\n\nfor cat in sorted_cats:\n    ids.append(cat)\n    labels.append(cat)\n    parents.append('Luxembourg .lu')\n    values.append(cat_totals[cat])\n    colors.append(cat_colors.get(cat, '#999'))\n    n = sum(1 for t in topics_2024 if t['sector'] == cat)\n    customdata.append(f'{cat_totals[cat]} websites across {n} topics')\n\nfor t in topics_2024:\n    ids.append(f\"{t['sector']}/{t['id']}\")\n    labels.append(t['label'])\n    parents.append(t['sector'])\n    values.append(t['count'])\n    colors.append(cat_colors.get(t['sector'], '#999'))\n    customdata.append(t['words'])\n\nfig_treemap = go.Figure(go.Treemap(\n    ids=ids,\n    labels=labels,\n    parents=parents,\n    values=values,\n    branchvalues='total',\n    marker=dict(colors=colors, line=dict(width=1.5, color='white')),\n    customdata=customdata,\n    hovertemplate='&lt;b&gt;%{label}&lt;/b&gt;&lt;br&gt;%{value} websites&lt;br&gt;%{customdata}&lt;extra&gt;&lt;/extra&gt;',\n    textinfo='label+value',\n    textfont=dict(size=13),\n    maxdepth=2,\n    pathbar=dict(textfont=dict(size=13))\n))\n\nfig_treemap.update_layout(\n    height=550,\n    margin=dict(t=30, r=10, b=10, l=10),\n    paper_bgcolor='white'\n)\n\nfig_treemap.show(config={'displayModeBar': False})\n\n\n                            \n                                            \n\n\n\nNote: Based on 4,629 classified websites in 2024. The remaining 3,153 websites (40.5%) were too unique to fit any cluster. Categories are manual groupings of BERTopic’s automatically discovered topics.\n\n\n\n\n\n\n\n\nTipKey Findings\n\n\n\nReal estate dominates Luxembourg’s web with 643 websites — more than finance and healthcare combined. In a country where housing prices rose by nearly 90% between 2015 and their peak in 2022 (source), it makes sense that appartement, m², and chambres are the most common vocabulary on .lu domains.\nInvestment funds are the second-largest single topic (277 websites), with keywords almost entirely in English: fund, investment, asset management, tax, equity — reflecting Luxembourg’s role as Europe’s largest fund administration centre.\nConstruction is the most fragmented category: 11 distinct topics from roofing (toiture, charpente) to tiling (carrelages, salle de bain), reflecting a highly specialized trade sector where each craft maintains its own web presence.\nThe topic keywords also reveal the multilingual character of Luxembourg: appartement, chauffage (French), Fenster, Fassade (German), Musek, Scouten, Haff (Luxembourgish), and fund, cloud (English) — all coexisting in the same .lu domain space, as explored in my previous analysis."
  },
  {
    "objectID": "posts/bert_topic_websites_lux/index.html#how-stable-is-this-landscape",
    "href": "posts/bert_topic_websites_lux/index.html#how-stable-is-this-landscape",
    "title": "What Does Luxembourg’s Web Talk About?",
    "section": "How Stable Is This Landscape?",
    "text": "How Stable Is This Landscape?\nThe treemap shows a snapshot of 2024, but has the composition always looked like this? Because the model runs globally across all years, each topic has a consistent identity — so I can track how the share of each category evolved from 2016 to 2024.\n\n\nShare of classified websites per category, 2016–2024\n\n\n\nShow code\nyears = sector_evo['years']\n\n# All named sectors sorted by latest-year share (largest first), then \"Other\"\nnamed = [s for s in sector_evo['sectors'] if s['name'] != 'Other']\nnamed_sorted = sorted(named, key=lambda s: -s['shares'][-1])\nother = [s for s in sector_evo['sectors'] if s['name'] == 'Other']\n\nfig = go.Figure()\n\n# Add named sectors (largest on bottom for stacked chart)\nfor sector in reversed(named_sorted):\n    fig.add_trace(go.Bar(\n        x=years,\n        y=sector['shares'],\n        name=sector['name'],\n        marker_color=cat_colors.get(sector['name'], '#999'),\n        hovertemplate=f\"&lt;b&gt;{sector['name']}&lt;/b&gt;&lt;br&gt;%{{y:.1f}}% of classified websites&lt;br&gt;%{{x}}&lt;extra&gt;&lt;/extra&gt;\"\n    ))\n\n# Add \"Other\" (unmapped topics) at the bottom\nif other:\n    fig.add_trace(go.Bar(\n        x=years,\n        y=other[0]['shares'],\n        name='Other',\n        marker_color=cat_colors.get('Other', '#e0e0e0'),\n        hovertemplate='&lt;b&gt;Other (unmapped topics)&lt;/b&gt;&lt;br&gt;%{y:.1f}%&lt;br&gt;%{x}&lt;extra&gt;&lt;/extra&gt;'\n    ))\n\nfig.update_layout(\n    height=500,\n    margin=dict(t=20, r=20, b=100, l=60),\n    plot_bgcolor='white',\n    paper_bgcolor='white',\n    barmode='stack',\n    legend=dict(orientation='h', y=-0.22, x=0.5, xanchor='center',\n                traceorder='normal'),\n    hovermode='x unified',\n    bargap=0.25\n)\n\nfig.update_xaxes(\n    title='Year', dtick=1,\n    gridcolor='#eee', zerolinecolor='#eee'\n)\nfig.update_yaxes(\n    title_text='Share of classified websites (%)',\n    gridcolor='#eee', zerolinecolor='#eee',\n    range=[0, 100]\n)\n\nfig.show(config={'displayModeBar': False})\n\n\n                            \n                                            \n\n\n\nNote: Each bar shows the share of classified websites belonging to each category. All 15 categories from the treemap are shown; “Other” covers the remaining smaller topics without a category assignment.\n\n\n\n\n\n\n\n\nTipKey Findings\n\n\n\nThe composition is remarkably stable. Real estate, finance, and construction have held their positions since 2016, reflecting an economy whose core pillars haven’t changed.\nThe one visible shift is data protection: barely present before 2018, it grew into a significant category after GDPR required businesses to publish privacy policies and cookie notices. This isn’t a new sector — it’s existing websites adding standardized legal text that the algorithm detects as a recurring pattern."
  },
  {
    "objectID": "posts/bert_topic_websites_lux/index.html#methodology",
    "href": "posts/bert_topic_websites_lux/index.html#methodology",
    "title": "What Does Luxembourg’s Web Talk About?",
    "section": "Methodology",
    "text": "Methodology\nA single global BERTopic model processes all 81,585 website-years at once, producing consistent topic IDs that can be tracked across years without heuristic matching.\n\n\n1. Data Preparation\n\nFrom the same CommonCrawl sample used in my language analysis (81,585 website-years, 2013–2024), I aggregated all crawled pages per website-year. Within each website, repeated paragraphs — navigation menus, footers, cookie banners — were deduplicated by comparing normalized text blocks across pages. This removed an average of 25% of text volume while preserving all unique content.\n\n2. Sentence Embeddings\n\nEach website’s deduplicated text was encoded using BAAI/bge-m3, a multilingual sentence transformer with a context window of 8,192 tokens. The 1,024-dimensional embeddings were computed on GPU (one SLURM array job per year).\n\n3. Global BERTopic\n\nAll embeddings across all years are combined into a single BERTopic run: UMAP reduces the 1,024 dimensions to 5, HDBSCAN clusters them (minimum cluster size = 50), and c-TF-IDF extracts keywords from the full text (up to 50,000 characters), using n-grams (1,2) with stopwords for English, French, German, Portuguese, Dutch, and Luxembourgish. BERTopic’s built-in topics_over_time() method then tracks how each topic’s prevalence changes year by year.\n\n4. Category Aggregation\n\nThe 191 automatically discovered topics were manually grouped into 15 categories (Real Estate, Finance & Law, Construction, etc.) based on their keywords and representative documents. This mapping covers the 94 largest topics. The remaining smaller topics are grouped as “Other”."
  },
  {
    "objectID": "posts/bert_topic_websites_lux/index.html#citation",
    "href": "posts/bert_topic_websites_lux/index.html#citation",
    "title": "What Does Luxembourg’s Web Talk About?",
    "section": "Citation",
    "text": "Citation\nFor attribution, please cite this work as:\nGarbers, J. (2026, February). What Does Luxembourg's Web Talk About?\nRetrieved from https://github.com/julio-garbers/blog/tree/main/bert_topic_websites_lux\nBibTeX:\n@misc{garbers2026topics,\n  author = {Garbers, Julio},\n  title = {What Does Luxembourg's Web Talk About?},\n  url = {https://github.com/julio-garbers/blog/tree/main/bert_topic_websites_lux},\n  year = {2026}\n}\n\n\n\n\n\nAll scripts available on GitHub"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "View CV (PDF)"
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "Experiments",
    "section": "",
    "text": "These online experiments are part of ongoing research on face perception. Each experiment is password protected. If you want to participate in this demo version, please send me an e-mail.\n\n\n\nFace Perception Study\nMeasures how accurately people identify race and facial expressions when viewing individual faces at different sizes.\nDuration: ~10 minutes\nStart Experiment\n\n\nFace Memory Study\nTests how well people remember the racial composition and expressions of faces shown in groups.\nDuration: ~15 minutes\nStart Experiment\n\n\nSubjective Traits Study\nMeasures how people rate traits like trustworthiness, competence, attractiveness, and friendliness based on profile pictures at different sizes.\nDuration: ~15 minutes\nStart Experiment"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "What Does Luxembourg’s Web Talk About?\n\n\nUsing BERTopic on CommonCrawl archives, I applied unsupervised topic modeling to Luxembourg websites. The results reveal what .lu domains talk about — from property listings and investment funds to sushi menus and scout camps.\n\n\n\n\n\nFebruary 23, 2026\n\n\n\n\n\n\n\nThe Linguistic Web of Luxembourg\n\n\nUsing CommonCrawl data, I analyzed 83,728 pages of Luxembourg websites from 2013 to 2024, detecting which languages each website offers. I explore language trends over time, multilingual patterns, and the most common language configurations.\n\n\n\n\n\nJanuary 17, 2026\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julio Garbers",
    "section": "",
    "text": "Contact: julio.garbers@liser.lu\n\nLuxembourg Institute of Socio-Economic Research (LISER) 11, Porte des Sciences Campus Belval L-4366 Esch-sur-Alzette www.liser.lu"
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Julio Garbers",
    "section": "Research Interests",
    "text": "Research Interests\n\nMigration\nDiscrimination\nPolicy Evaluation\nLabor Economics"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Julio Garbers",
    "section": "Education",
    "text": "Education\n\nPhD in Economics (2021 – 2025) LISER and University of Luxembourg\n\n\nMaster’s Degree in Economics (2018 – 2021) University of Heidelberg\n\n\nBachelor’s Degree in Economics (2015 – 2018) University of Heidelberg"
  },
  {
    "objectID": "posts/website_languages_lux/index.html",
    "href": "posts/website_languages_lux/index.html",
    "title": "The Linguistic Web of Luxembourg",
    "section": "",
    "text": "Using CommonCrawl data, I analyzed 83,728 pages of Luxembourg websites from 2013 to 2024, detecting which languages each website offers. Below, I explore language trends over time, multilingual patterns, and the most common language configurations."
  },
  {
    "objectID": "posts/website_languages_lux/index.html#the-language-landscape",
    "href": "posts/website_languages_lux/index.html#the-language-landscape",
    "title": "The Linguistic Web of Luxembourg",
    "section": "The Language Landscape",
    "text": "The Language Landscape\nWhich languages do Luxembourg websites make available to visitors?\n\n\nLanguage availability over time (% of websites offering each language)\n\n\n\nShow code\nfig = go.Figure()\n\nfor lang in ['fr', 'de', 'en', 'lb', 'pt', 'nl', 'other']:\n    fig.add_trace(go.Scatter(\n        x=yearly['year'].to_list(),\n        y=yearly[f'{lang}_pct'].to_list(),\n        name=lang_names[lang],\n        mode='lines+markers',\n        line=dict(color=lang_colors[lang], width=3, shape='spline', smoothing=0.8),\n        marker=dict(size=6),\n        hovertemplate=f\"&lt;b&gt;{lang_names[lang]}&lt;/b&gt;&lt;br&gt;%{{y:.1f}}% of websites&lt;br&gt;Year: %{{x}}&lt;extra&gt;&lt;/extra&gt;\"\n    ))\n\nfig.update_layout(\n    height=450,\n    margin=dict(t=20, r=20, b=100, l=60),\n    plot_bgcolor=\"white\",\n    paper_bgcolor=\"white\",\n    xaxis=dict(title=\"Year\", dtick=1, gridcolor=\"#eee\", zerolinecolor=\"#eee\"),\n    yaxis=dict(title=\"% of websites\", ticksuffix=\"%\", gridcolor=\"#eee\", zerolinecolor=\"#eee\"),\n    legend=dict(orientation=\"h\", y=-0.28, x=0.5, xanchor=\"center\"),\n    hovermode=\"x unified\"\n)\n\nfig.show(config={'displayModeBar': False})\n\n\n                            \n                                            \n\n\n\nNote: Percentages indicate share of websites offering each language. Sites can offer multiple languages, so percentages do not sum to 100%.\n\n\n\n\n\n\n\n\nTipKey Findings\n\n\n\nFrench dominates at 75% and has remained remarkably stable over the decade. I don’t have a good explanation for the sharp 2019 decrease in German, English, and Luxembourgish on websites.\nThe increase in websites in Luxembourgish from 2016–2018 coincides with the government’s “Strategie fir d’Promotioun vun der Lëtzebuerger Sprooch”, launched in March 2017. This plan aimed to increase digital presence and standardization of Luxembourgish, culminating in the July 2018 law establishing a Language Commissioner and the Centre for the Luxembourgish Language. (source)\nPortuguese speakers make up 14.5% of Luxembourg’s population — the largest immigrant community — but only 2.4% of websites offer Portuguese content. (source)"
  },
  {
    "objectID": "posts/website_languages_lux/index.html#beyond-monolingual",
    "href": "posts/website_languages_lux/index.html#beyond-monolingual",
    "title": "The Linguistic Web of Luxembourg",
    "section": "Beyond Monolingual",
    "text": "Beyond Monolingual\nIs Luxembourg’s constitutional trilingualism (French, German, Luxembourgish) reflected online?\n\n\nMultilingual breakdown over time\n\n\n\nShow code\ncolors = ['rgba(59, 130, 246, 0.5)', 'rgba(16, 185, 129, 0.5)', 'rgba(245, 158, 11, 0.5)', 'rgba(239, 68, 68, 0.5)']\nline_colors = ['#3b82f6', '#10b981', '#f59e0b', '#ef4444']\nnames = ['1 Language', '2 Languages', '3 Languages', '4+ Languages']\nkeys = ['monolingual', 'bilingual', 'trilingual', 'quadlingual_plus']\n\nfig = go.Figure()\n\nfor i, (key, name, color, line_color) in enumerate(zip(keys, names, colors, line_colors)):\n    fig.add_trace(go.Scatter(\n        x=multilingual['year'].to_list(),\n        y=multilingual[key].to_list(),\n        name=name,\n        mode='lines',\n        stackgroup='one',\n        fillcolor=color,\n        line=dict(color=line_color, width=0),\n        hovertemplate=f\"&lt;b&gt;{name}&lt;/b&gt;&lt;br&gt;%{{y:.1f}}%&lt;extra&gt;&lt;/extra&gt;\"\n    ))\n\nfig.update_layout(\n    height=450,\n    margin=dict(t=20, r=20, b=100, l=60),\n    plot_bgcolor=\"white\",\n    paper_bgcolor=\"white\",\n    xaxis=dict(title=\"Year\", dtick=1, gridcolor=\"#eee\", zerolinecolor=\"#eee\"),\n    yaxis=dict(title=\"% of websites\", ticksuffix=\"%\", range=[0, 100], gridcolor=\"#eee\", zerolinecolor=\"#eee\", tickvals=[20, 40, 60, 80, 100], ticktext=[\"20%\", \"40%\", \"60%\", \"80%\", \"100%\"]),\n    legend=dict(orientation=\"h\", y=-0.28, x=0.5, xanchor=\"center\"),\n    hovermode=\"x unified\"\n)\n\nfig.show(config={'displayModeBar': False})\n\n\n                            \n                                            \n\n\n\nNote: Stacked area chart shows the percentage of websites offering 1, 2, 3, or 4+ languages.\n\n\n\n\n\n\n\n\nTipKey Findings\n\n\n\nContrary to what one might expect, the share of multilingual websites has slightly decreased — from 46% in 2013 to 41% in 2024.\nThe majority of Luxembourg websites (59%) remain monolingual, with French-only sites being the most common configuration."
  },
  {
    "objectID": "posts/website_languages_lux/index.html#language-configurations",
    "href": "posts/website_languages_lux/index.html#language-configurations",
    "title": "The Linguistic Web of Luxembourg",
    "section": "Language Configurations",
    "text": "Language Configurations\nWhat are the most common language configurations?\n\n\nMost common language configurations (2024)\n\n\n\nShow code\n# Reverse for horizontal bar chart\ncombos_reversed = combinations.reverse()\n\n# Create abbreviated labels for mobile (single letter, no spaces)\nabbrev_map = {\n    'French': 'F', 'German': 'D', 'English': 'E',\n    'Luxembourgish': 'L', 'Portuguese': 'P', 'Dutch': 'N'\n}\n\ndef abbreviate(combo):\n    result = combo\n    for full, short in abbrev_map.items():\n        result = result.replace(full, short)\n    return result.replace(' + ', '+')\n\nfull_labels = combos_reversed['combo'].to_list()\nshort_labels = [abbreviate(c) for c in full_labels]\n\ncolors_gradient = [\n    f'hsl({200 + i * 10}, 60%, {45 + i * 3}%)'\n    for i in range(len(combos_reversed))\n]\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    y=full_labels,\n    x=combos_reversed['pct'].to_list(),\n    orientation='h',\n    marker=dict(color=colors_gradient),\n    text=[f\"{p}%\" for p in combos_reversed['pct'].to_list()],\n    textposition='outside',\n    hovertemplate=\"&lt;b&gt;%{y}&lt;/b&gt;&lt;br&gt;%{x:.1f}% of websites&lt;extra&gt;&lt;/extra&gt;\"\n))\n\nfig.update_layout(\n    height=500,\n    margin=dict(t=20, r=80, b=50, l=200),\n    plot_bgcolor=\"white\",\n    paper_bgcolor=\"white\",\n    xaxis=dict(title=\"% of websites\", ticksuffix=\"%\", range=[0, 48], gridcolor=\"#eee\", zerolinecolor=\"#eee\"),\n    yaxis=dict(tickfont=dict(size=10), gridcolor=\"#eee\"),\n    showlegend=False\n)\n\nfig.show(config={'displayModeBar': False})\n\n\n                            \n                                            \n\n\n\n\nNote: Based on 8,061 websites in 2024. Only the four main languages (French, German, English, Luxembourgish) are shown.\n\n\n\n\n\n\n\n\nTipKey Findings\n\n\n\nFrench-only sites dominate at 38.1%, followed by English-only at 12.9%, even though English isn’t an official language.\nThe French + German pairing (8.6%) has been overtaken by French + English (10.8%).\nThe full quadrilingual setup (French + German + English + Luxembourgish) remains rare at just 2.4%.\nNotably, Luxembourgish-only sites sit at the bottom (1.2%) — the national language rarely stands alone online, almost always appearing alongside French or German."
  },
  {
    "objectID": "posts/website_languages_lux/index.html#methodology",
    "href": "posts/website_languages_lux/index.html#methodology",
    "title": "The Linguistic Web of Luxembourg",
    "section": "Methodology",
    "text": "Methodology\nThis analysis detects language availability — which languages a website offers to visitors — rather than just language content (what language a page happens to be written in). This distinction matters because analyzing page content alone only reveals the language of that specific page, not the full set of languages a site offers. By detecting language switchers and hreflang tags, I can identify multilingual sites even when only one language version was archived.\nI use a three-tier detection pipeline, prioritizing the most reliable signals first:\n\n\n1. Data Collection\n\nI extracted 83,728 website-year observations from CommonCrawl archives (2013–2024), filtering for .lu domains. CommonCrawl provides free, publicly available web archives — enabling reproducible research without requiring custom crawling infrastructure.\n\n2. Hreflang Extraction\n\nI scanned HTML for hreflang tags — the W3C standard for declaring language alternatives. When present, these tags explicitly list all language versions a site offers, making them the most reliable signal. Found in 15,808 website-years (19%).\n\n3. LLM Detection\n\nFor sites without hreflang tags, I used Mistral’s Magistral model to detect language switchers in HTML navigation elements. LLMs can identify patterns like “FR | DE | EN” menus that rule-based approaches miss. Applied to 67,774 website-years (81%).\n\n4. FastText Fallback\n\nSites with no detected language switcher are assumed monolingual. I classified their primary language using FastText, a lightweight model trained on Wikipedia. Applied to 4,995 website-years (6%).\n\n\n\n\nNote: 1,134 website-years (1.4%) had insufficient text for language classification and were excluded from the analysis."
  },
  {
    "objectID": "posts/website_languages_lux/index.html#citation",
    "href": "posts/website_languages_lux/index.html#citation",
    "title": "The Linguistic Web of Luxembourg",
    "section": "Citation",
    "text": "Citation\nFor attribution, please cite this work as:\nGarbers, J. (2026, January). The Linguistic Web of Luxembourg.\nRetrieved from https://github.com/julio-garbers/blog/tree/main/website_languages_lux\nBibTeX:\n@misc{garbers2026linguistic,\n  author = {Garbers, Julio},\n  title = {The Linguistic Web of Luxembourg},\n  url = {https://github.com/julio-garbers/blog/tree/main/website_languages_lux},\n  year = {2026}\n}\n\n\n\n\n\nAll scripts available on GitHub"
  },
  {
    "objectID": "policy.html",
    "href": "policy.html",
    "title": "Policy",
    "section": "",
    "text": "with Christina Gathmann and Felix Stips Wirtschaftsdienst, 2025\n\n\nDeutschland ist in den letzten Jahren zu einem wichtigen Zielland für internationale Fachkräfte geworden. Herkömmliche Datenquellen wie die amtliche Statistik oder der Mikrozensus erfassen jedoch nicht, welche Kompetenzen hochqualifizierte Zuwanderer mitbringen oder durch Abwanderung verloren gehen. Mithilfe von LinkedIn-Daten machen wir diesen Kompetenzfluss sichtbar und zeigen: Deutschland ist ein deutlicher Nettoimporteur von Fähigkeiten – insbesondere in den Bereichen IT, kognitive Kompetenzen sowie Projekt- und Personalmanagement. Sowohl Erwerbs- als auch Bildungsmigration tragen hierzu bei, wobei die Bildungsmigration durch die Pandemie stärker beeinträchtigt wurde. Entscheidend ist: Deutschlands Wettbewerbsfähigkeit hängt maßgeblich von einer aktiven Fachkräftepolitik sowie von attraktiven Ausbildungs- und Bleibeperspektiven ab.\n\n\nReport (PDF)"
  },
  {
    "objectID": "policy.html#reports",
    "href": "policy.html#reports",
    "title": "Policy",
    "section": "",
    "text": "with Christina Gathmann and Felix Stips Wirtschaftsdienst, 2025\n\n\nDeutschland ist in den letzten Jahren zu einem wichtigen Zielland für internationale Fachkräfte geworden. Herkömmliche Datenquellen wie die amtliche Statistik oder der Mikrozensus erfassen jedoch nicht, welche Kompetenzen hochqualifizierte Zuwanderer mitbringen oder durch Abwanderung verloren gehen. Mithilfe von LinkedIn-Daten machen wir diesen Kompetenzfluss sichtbar und zeigen: Deutschland ist ein deutlicher Nettoimporteur von Fähigkeiten – insbesondere in den Bereichen IT, kognitive Kompetenzen sowie Projekt- und Personalmanagement. Sowohl Erwerbs- als auch Bildungsmigration tragen hierzu bei, wobei die Bildungsmigration durch die Pandemie stärker beeinträchtigt wurde. Entscheidend ist: Deutschlands Wettbewerbsfähigkeit hängt maßgeblich von einer aktiven Fachkräftepolitik sowie von attraktiven Ausbildungs- und Bleibeperspektiven ab.\n\n\nReport (PDF)"
  },
  {
    "objectID": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html",
    "href": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html",
    "title": "Google Sheets Configuration",
    "section": "",
    "text": "Both experiments send data to the same Google Sheets file but to different sheets.\n\n\n\n\n\n\n\n\n\n\nSheet Tab\nExperiment\nContent\n\n\n\n\natt_acc\nExperiment 1 (Attention/Accuracy)\n16 trials per participant\n\n\nmemory\nExperiment 2 (Memory)\n12 rounds per participant\n\n\ndebug\nBoth\nDebug logging\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntimestamp\nWhen data was submitted\n\n\nparticipant_id\nUnique ID\n\n\ndemo_age\nParticipant age\n\n\ndemo_gender\nParticipant gender\n\n\ndemo_race\nParticipant race/ethnicity\n\n\ndemo_education\nEducation level\n\n\ntrial_number\n1-16 or “practice”\n\n\nimage_name\nFull image filename without extension (e.g., “black_male_smile_01_big”)\n\n\ntrue_race / true_gender\nGround truth\n\n\nsize_condition\n“big” or “small”\n\n\nsmile_condition\n“smile” or “nosmile”\n\n\nquestion_order\n“race_first” or “smile_first”\n\n\nrace_options_order\nOrder of race buttons (e.g., “asian,black,hispanic,white”)\n\n\nsmile_options_order\nOrder of smile buttons (e.g., “yes,no”)\n\n\nrace_response / race_rt / race_correct\nRace question data\n\n\nsmile_response / smile_rt / smile_correct\nSmile question data\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntimestamp\nWhen data was submitted\n\n\nparticipant_id\nUnique ID\n\n\ndemo_age\nParticipant age\n\n\ndemo_gender\nParticipant gender\n\n\ndemo_race\nParticipant race/ethnicity\n\n\ndemo_education\nEducation level\n\n\nround_number\n1-12 or “practice”\n\n\nsize_condition\n“big” or “small”\n\n\nquestion_type\n“race” or “smile”\n\n\ngrid_order\nOrder of images in grid positions 1-8 (e.g., “black_male_01:smile,asian_female_02:nosmile,…”)\n\n\ninput_order\nOrder of input fields shown (e.g., “hispanic,black,white,asian”)\n\n\nresponse_order\nOrder participant filled in inputs (e.g., “black,white,hispanic,asian”)\n\n\nactual_*\nTrue count in the grid\n\n\n*_response\nParticipant’s count\n\n\n*_error\nResponse minus actual (negative = undercount)\n\n\n*_correct\nTRUE/FALSE whether response matches actual\n\n\nresponse_rt\nResponse time in ms\n\n\n\n\n\n\n\n\nThe following script is deployed in Google Sheets to handle data from both experiments.\n/**\n * Google Apps Script for Face Perception Experiments\n *\n * Handles data from:\n * - Experiment 1: Attention/Accuracy → \"att_acc\" sheet\n * - Experiment 2: Memory → \"memory\" sheet\n * - Debug logging → \"debug\" sheet\n */\n\n// ============================================================================\n// CONFIGURATION - Sheet names for each experiment\n// ============================================================================\n\nconst SHEET_NAMES = {\n    attention: 'att_acc',   // Experiment 1: Attention/Accuracy\n    memory: 'memory'        // Experiment 2: Memory\n};\n\n// ============================================================================\n// MAIN REQUEST HANDLERS\n// ============================================================================\n\n/**\n * Handle POST requests from experiments\n */\nfunction doPost(e) {\n    const ss = SpreadsheetApp.getActiveSpreadsheet();\n\n    // Get or create debug sheet for logging\n    let debugSheet = ss.getSheetByName('debug');\n    if (!debugSheet) {\n        debugSheet = ss.insertSheet('debug');\n    }\n\n    // Log what we received\n    debugSheet.appendRow([new Date(), 'POST received']);\n    debugSheet.appendRow([new Date(), 'e.parameter', JSON.stringify(e.parameter || {})]);\n    debugSheet.appendRow([new Date(), 'e.postData', e.postData ? 'exists' : 'null']);\n\n    if (e.postData) {\n        debugSheet.appendRow([new Date(), 'e.postData.type', e.postData.type || 'no type']);\n        debugSheet.appendRow([new Date(), 'e.postData.contents length', e.postData.contents ? e.postData.contents.length : 0]);\n    }\n\n    try {\n        // Parse incoming data\n        let data;\n        if (e.parameter && e.parameter.data) {\n            debugSheet.appendRow([new Date(), 'Parsing from e.parameter.data']);\n            data = JSON.parse(e.parameter.data);\n        } else if (e.postData && e.postData.contents) {\n            debugSheet.appendRow([new Date(), 'Parsing from e.postData.contents']);\n            data = JSON.parse(e.postData.contents);\n        } else {\n            debugSheet.appendRow([new Date(), 'ERROR', 'No data found']);\n            throw new Error('No data received');\n        }\n\n        // Determine which experiment sent the data\n        // Default to 'attention' for backwards compatibility with Experiment 1\n        const experimentType = data.experiment || 'attention';\n        const sheetName = SHEET_NAMES[experimentType] || SHEET_NAMES.attention;\n\n        debugSheet.appendRow([new Date(), 'SUCCESS', 'experiment: ' + experimentType + ', participant_id: ' + data.participant_id]);\n\n        // Get or create the appropriate sheet\n        let sheet = ss.getSheetByName(sheetName);\n\n        if (!sheet) {\n            // Create the sheet if it doesn't exist\n            sheet = ss.insertSheet(sheetName);\n            if (experimentType === 'memory') {\n                addMemoryHeaders(sheet);\n            } else {\n                addAttentionHeaders(sheet);\n            }\n        }\n\n        // Add headers if sheet is empty\n        if (sheet.getLastRow() === 0) {\n            if (experimentType === 'memory') {\n                addMemoryHeaders(sheet);\n            } else {\n                addAttentionHeaders(sheet);\n            }\n        }\n\n        // Append the data to the appropriate sheet\n        if (experimentType === 'memory') {\n            appendMemoryData(sheet, data);\n        } else {\n            appendAttentionData(sheet, data);\n        }\n\n        // Return success response\n        return ContentService\n            .createTextOutput(JSON.stringify({\n                status: 'success',\n                experiment: experimentType,\n                sheet: sheetName\n            }))\n            .setMimeType(ContentService.MimeType.JSON);\n\n    } catch (error) {\n        debugSheet.appendRow([new Date(), 'ERROR', error.toString()]);\n        return ContentService\n            .createTextOutput(JSON.stringify({\n                status: 'error',\n                message: error.toString()\n            }))\n            .setMimeType(ContentService.MimeType.JSON);\n    }\n}\n\n/**\n * Handle GET requests (for testing the endpoint)\n */\nfunction doGet(e) {\n    return ContentService\n        .createTextOutput('Face Perception Experiments Data Collector v2.0\\n\\nSupported experiments:\\n- attention → att_acc sheet\\n- memory → memory sheet')\n        .setMimeType(ContentService.MimeType.TEXT);\n}\n\n// ============================================================================\n// EXPERIMENT 1: ATTENTION/ACCURACY\n// ============================================================================\n\n/**\n * Add headers for the Attention/Accuracy sheet\n */\nfunction addAttentionHeaders(sheet) {\n    const headers = [\n        'timestamp',\n        'participant_id',\n        // Demographics\n        'demo_age',\n        'demo_gender',\n        'demo_race',\n        'demo_education',\n        // Zoom tracking\n        'zoom_check_bypassed',\n        'zoom_check_attempts',\n        'approved_dpr',\n        'zoom_changes_count',\n        'zoom_changes',\n        'terminated_due_to_zoom',\n        // Trial data\n        'trial_number',\n        'image_name',\n        'true_race',\n        'true_gender',\n        'size_condition',\n        'smile_condition',\n        'question_order',\n        'race_options_order',\n        'smile_options_order',\n        'race_response',\n        'race_rt',\n        'race_correct',\n        'smile_response',\n        'smile_rt',\n        'smile_correct',\n        'is_practice'\n    ];\n\n    sheet.getRange(1, 1, 1, headers.length).setValues([headers]);\n    sheet.getRange(1, 1, 1, headers.length).setFontWeight('bold');\n    sheet.setFrozenRows(1);\n}\n\n/**\n * Append data from Experiment 1\n */\nfunction appendAttentionData(sheet, data) {\n    const timestamp = data.timestamp || new Date().toISOString();\n    const participantId = data.participant_id || '';\n    const demographics = data.demographics || {};\n    const zoomTracking = data.zoom_tracking || {};\n    const trials = data.trials || [];\n\n    trials.forEach(trial =&gt; {\n        const row = [\n            timestamp,\n            participantId,\n            // Demographics\n            demographics.age || '',\n            demographics.gender || '',\n            demographics.race || '',\n            demographics.education || '',\n            // Zoom tracking\n            zoomTracking.zoom_check_bypassed || false,\n            zoomTracking.zoom_check_attempts || 0,\n            zoomTracking.approved_dpr || '',\n            zoomTracking.zoom_changes_count || 0,\n            zoomTracking.zoom_changes ? JSON.stringify(zoomTracking.zoom_changes) : '',\n            zoomTracking.terminated_due_to_zoom || false,\n            // Trial data\n            trial.trial_number,\n            trial.image_name || '',\n            trial.true_race,\n            trial.true_gender,\n            trial.size_condition,\n            trial.smile_condition,\n            trial.question_order,\n            trial.race_options_order || '',\n            trial.smile_options_order || '',\n            trial.race_response,\n            trial.race_rt,\n            trial.race_correct,\n            trial.smile_response,\n            trial.smile_rt,\n            trial.smile_correct,\n            trial.is_practice\n        ];\n        sheet.appendRow(row);\n    });\n}\n\n// ============================================================================\n// EXPERIMENT 2: MEMORY\n// ============================================================================\n\n/**\n * Add headers for the memory sheet\n */\nfunction addMemoryHeaders(sheet) {\n    const headers = [\n        'timestamp',\n        'participant_id',\n        // Demographics\n        'demo_age',\n        'demo_gender',\n        'demo_race',\n        'demo_education',\n        // Zoom tracking\n        'zoom_check_bypassed',\n        'zoom_check_attempts',\n        'approved_dpr',\n        'zoom_changes_count',\n        'zoom_changes',\n        'terminated_due_to_zoom',\n        // Round info\n        'round_number',\n        'size_condition',\n        'question_type',\n        'is_practice',\n        'grid_order',\n        'input_order',\n        'response_order',\n        // Actual composition (ground truth)\n        'actual_asian',\n        'actual_black',\n        'actual_hispanic',\n        'actual_white',\n        'actual_smiling',\n        'actual_not_smiling',\n        // Race question responses\n        'asian_response',\n        'black_response',\n        'hispanic_response',\n        'white_response',\n        'asian_error',\n        'black_error',\n        'hispanic_error',\n        'white_error',\n        'asian_correct',\n        'black_correct',\n        'hispanic_correct',\n        'white_correct',\n        // Smile question responses\n        'smiling_response',\n        'not_smiling_response',\n        'smiling_error',\n        'not_smiling_error',\n        'smiling_correct',\n        'not_smiling_correct',\n        // Timing\n        'response_rt'\n    ];\n\n    sheet.getRange(1, 1, 1, headers.length).setValues([headers]);\n    sheet.getRange(1, 1, 1, headers.length).setFontWeight('bold');\n    sheet.setFrozenRows(1);\n}\n\n/**\n * Append data from Experiment 2\n */\nfunction appendMemoryData(sheet, data) {\n    const timestamp = data.timestamp || new Date().toISOString();\n    const participantId = data.participant_id || '';\n    const demographics = data.demographics || {};\n    const zoomTracking = data.zoom_tracking || {};\n    const rounds = data.rounds || [];\n\n    rounds.forEach(round =&gt; {\n        const row = [\n            timestamp,\n            participantId,\n            // Demographics\n            demographics.age || '',\n            demographics.gender || '',\n            demographics.race || '',\n            demographics.education || '',\n            // Zoom tracking\n            zoomTracking.zoom_check_bypassed || false,\n            zoomTracking.zoom_check_attempts || 0,\n            zoomTracking.approved_dpr || '',\n            zoomTracking.zoom_changes_count || 0,\n            zoomTracking.zoom_changes ? JSON.stringify(zoomTracking.zoom_changes) : '',\n            zoomTracking.terminated_due_to_zoom || false,\n            // Round info\n            round.round_number,\n            round.size_condition,\n            round.question_type,\n            round.is_practice,\n            round.grid_order || '',\n            round.input_order || '',\n            round.response_order || '',\n            // Actual composition\n            round.actual_asian,\n            round.actual_black,\n            round.actual_hispanic,\n            round.actual_white,\n            round.actual_smiling,\n            round.actual_not_smiling,\n            // Race responses (empty if smile question)\n            round.asian_response !== undefined ? round.asian_response : '',\n            round.black_response !== undefined ? round.black_response : '',\n            round.hispanic_response !== undefined ? round.hispanic_response : '',\n            round.white_response !== undefined ? round.white_response : '',\n            round.asian_error !== undefined ? round.asian_error : '',\n            round.black_error !== undefined ? round.black_error : '',\n            round.hispanic_error !== undefined ? round.hispanic_error : '',\n            round.white_error !== undefined ? round.white_error : '',\n            round.asian_correct !== undefined ? round.asian_correct : '',\n            round.black_correct !== undefined ? round.black_correct : '',\n            round.hispanic_correct !== undefined ? round.hispanic_correct : '',\n            round.white_correct !== undefined ? round.white_correct : '',\n            // Smile responses (empty if race question)\n            round.smiling_response !== undefined ? round.smiling_response : '',\n            round.not_smiling_response !== undefined ? round.not_smiling_response : '',\n            round.smiling_error !== undefined ? round.smiling_error : '',\n            round.not_smiling_error !== undefined ? round.not_smiling_error : '',\n            round.smiling_correct !== undefined ? round.smiling_correct : '',\n            round.not_smiling_correct !== undefined ? round.not_smiling_correct : '',\n            // Timing\n            round.response_rt\n        ];\n        sheet.appendRow(row);\n    });\n}\n\n// ============================================================================\n// TEST FUNCTIONS\n// ============================================================================\n\n/**\n * Test Experiment 1 (Attention/Accuracy)\n */\nfunction testAttentionExperiment() {\n    const testData = {\n        experiment: 'attention',\n        participant_id: 'TEST_ATT_' + new Date().getTime(),\n        timestamp: new Date().toISOString(),\n        demographics: {\n            age: '25',\n            gender: 'female',\n            education: 'bachelors'\n        },\n        trials: [\n            {\n                trial_number: 'practice',\n                image_name: 'white_female_smile_01_big',\n                true_race: 'white',\n                true_gender: 'female',\n                size_condition: 'big',\n                smile_condition: 'smile',\n                question_order: 'race_first',\n                race_options_order: 'white,black,asian,hispanic',\n                smile_options_order: 'yes,no',\n                race_response: 'white',\n                race_rt: 1500,\n                race_correct: true,\n                smile_response: 'yes',\n                smile_rt: 1200,\n                smile_correct: true,\n                is_practice: true\n            }\n        ]\n    };\n\n    const mockEvent = {\n        postData: { contents: JSON.stringify(testData) }\n    };\n\n    const result = doPost(mockEvent);\n    console.log('Attention Test Result:', result.getContent());\n}\n\n/**\n * Test Experiment 2 (Memory)\n */\nfunction testMemoryExperiment() {\n    const testData = {\n        experiment: 'memory',\n        participant_id: 'TEST_MEM_' + new Date().getTime(),\n        timestamp: new Date().toISOString(),\n        demographics: {\n            age: '28',\n            gender: 'male',\n            education: 'masters'\n        },\n        rounds: [\n            {\n                round_number: 'practice',\n                size_condition: 'big',\n                question_type: 'race',\n                is_practice: true,\n                grid_order: 'black_male_01:smile,asian_female_01:nosmile,white_male_01:smile,hispanic_female_01:nosmile,black_female_01:smile,asian_male_01:nosmile,white_female_01:smile,hispanic_male_01:nosmile',\n                input_order: 'hispanic,black,white,asian',\n                response_order: 'black,hispanic,white,asian',\n                actual_asian: 2,\n                actual_black: 2,\n                actual_hispanic: 2,\n                actual_white: 2,\n                actual_smiling: 4,\n                actual_not_smiling: 4,\n                asian_response: 2,\n                black_response: 2,\n                hispanic_response: 2,\n                white_response: 2,\n                asian_error: 0,\n                black_error: 0,\n                hispanic_error: 0,\n                white_error: 0,\n                asian_correct: true,\n                black_correct: true,\n                hispanic_correct: true,\n                white_correct: true,\n                response_rt: 15000\n            }\n        ]\n    };\n\n    const mockEvent = {\n        postData: { contents: JSON.stringify(testData) }\n    };\n\n    const result = doPost(mockEvent);\n    console.log('Memory Test Result:', result.getContent());\n}\n\n\n\n\n\n\n\nMake sure you deployed a new version of the script after changes\nRun the test functions to manually create sheets\n\n\n\n\n\nGo to Apps Script → Executions to see error logs\nCheck the debug sheet for detailed request logging"
  },
  {
    "objectID": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html#sheet-structure",
    "href": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html#sheet-structure",
    "title": "Google Sheets Configuration",
    "section": "",
    "text": "Sheet Tab\nExperiment\nContent\n\n\n\n\natt_acc\nExperiment 1 (Attention/Accuracy)\n16 trials per participant\n\n\nmemory\nExperiment 2 (Memory)\n12 rounds per participant\n\n\ndebug\nBoth\nDebug logging"
  },
  {
    "objectID": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html#data-columns-reference",
    "href": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html#data-columns-reference",
    "title": "Google Sheets Configuration",
    "section": "",
    "text": "Column\nDescription\n\n\n\n\ntimestamp\nWhen data was submitted\n\n\nparticipant_id\nUnique ID\n\n\ndemo_age\nParticipant age\n\n\ndemo_gender\nParticipant gender\n\n\ndemo_race\nParticipant race/ethnicity\n\n\ndemo_education\nEducation level\n\n\ntrial_number\n1-16 or “practice”\n\n\nimage_name\nFull image filename without extension (e.g., “black_male_smile_01_big”)\n\n\ntrue_race / true_gender\nGround truth\n\n\nsize_condition\n“big” or “small”\n\n\nsmile_condition\n“smile” or “nosmile”\n\n\nquestion_order\n“race_first” or “smile_first”\n\n\nrace_options_order\nOrder of race buttons (e.g., “asian,black,hispanic,white”)\n\n\nsmile_options_order\nOrder of smile buttons (e.g., “yes,no”)\n\n\nrace_response / race_rt / race_correct\nRace question data\n\n\nsmile_response / smile_rt / smile_correct\nSmile question data\n\n\n\n\n\n\n\n\n\n\n\n\n\nColumn\nDescription\n\n\n\n\ntimestamp\nWhen data was submitted\n\n\nparticipant_id\nUnique ID\n\n\ndemo_age\nParticipant age\n\n\ndemo_gender\nParticipant gender\n\n\ndemo_race\nParticipant race/ethnicity\n\n\ndemo_education\nEducation level\n\n\nround_number\n1-12 or “practice”\n\n\nsize_condition\n“big” or “small”\n\n\nquestion_type\n“race” or “smile”\n\n\ngrid_order\nOrder of images in grid positions 1-8 (e.g., “black_male_01:smile,asian_female_02:nosmile,…”)\n\n\ninput_order\nOrder of input fields shown (e.g., “hispanic,black,white,asian”)\n\n\nresponse_order\nOrder participant filled in inputs (e.g., “black,white,hispanic,asian”)\n\n\nactual_*\nTrue count in the grid\n\n\n*_response\nParticipant’s count\n\n\n*_error\nResponse minus actual (negative = undercount)\n\n\n*_correct\nTRUE/FALSE whether response matches actual\n\n\nresponse_rt\nResponse time in ms"
  },
  {
    "objectID": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html#google-apps-script",
    "href": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html#google-apps-script",
    "title": "Google Sheets Configuration",
    "section": "",
    "text": "The following script is deployed in Google Sheets to handle data from both experiments.\n/**\n * Google Apps Script for Face Perception Experiments\n *\n * Handles data from:\n * - Experiment 1: Attention/Accuracy → \"att_acc\" sheet\n * - Experiment 2: Memory → \"memory\" sheet\n * - Debug logging → \"debug\" sheet\n */\n\n// ============================================================================\n// CONFIGURATION - Sheet names for each experiment\n// ============================================================================\n\nconst SHEET_NAMES = {\n    attention: 'att_acc',   // Experiment 1: Attention/Accuracy\n    memory: 'memory'        // Experiment 2: Memory\n};\n\n// ============================================================================\n// MAIN REQUEST HANDLERS\n// ============================================================================\n\n/**\n * Handle POST requests from experiments\n */\nfunction doPost(e) {\n    const ss = SpreadsheetApp.getActiveSpreadsheet();\n\n    // Get or create debug sheet for logging\n    let debugSheet = ss.getSheetByName('debug');\n    if (!debugSheet) {\n        debugSheet = ss.insertSheet('debug');\n    }\n\n    // Log what we received\n    debugSheet.appendRow([new Date(), 'POST received']);\n    debugSheet.appendRow([new Date(), 'e.parameter', JSON.stringify(e.parameter || {})]);\n    debugSheet.appendRow([new Date(), 'e.postData', e.postData ? 'exists' : 'null']);\n\n    if (e.postData) {\n        debugSheet.appendRow([new Date(), 'e.postData.type', e.postData.type || 'no type']);\n        debugSheet.appendRow([new Date(), 'e.postData.contents length', e.postData.contents ? e.postData.contents.length : 0]);\n    }\n\n    try {\n        // Parse incoming data\n        let data;\n        if (e.parameter && e.parameter.data) {\n            debugSheet.appendRow([new Date(), 'Parsing from e.parameter.data']);\n            data = JSON.parse(e.parameter.data);\n        } else if (e.postData && e.postData.contents) {\n            debugSheet.appendRow([new Date(), 'Parsing from e.postData.contents']);\n            data = JSON.parse(e.postData.contents);\n        } else {\n            debugSheet.appendRow([new Date(), 'ERROR', 'No data found']);\n            throw new Error('No data received');\n        }\n\n        // Determine which experiment sent the data\n        // Default to 'attention' for backwards compatibility with Experiment 1\n        const experimentType = data.experiment || 'attention';\n        const sheetName = SHEET_NAMES[experimentType] || SHEET_NAMES.attention;\n\n        debugSheet.appendRow([new Date(), 'SUCCESS', 'experiment: ' + experimentType + ', participant_id: ' + data.participant_id]);\n\n        // Get or create the appropriate sheet\n        let sheet = ss.getSheetByName(sheetName);\n\n        if (!sheet) {\n            // Create the sheet if it doesn't exist\n            sheet = ss.insertSheet(sheetName);\n            if (experimentType === 'memory') {\n                addMemoryHeaders(sheet);\n            } else {\n                addAttentionHeaders(sheet);\n            }\n        }\n\n        // Add headers if sheet is empty\n        if (sheet.getLastRow() === 0) {\n            if (experimentType === 'memory') {\n                addMemoryHeaders(sheet);\n            } else {\n                addAttentionHeaders(sheet);\n            }\n        }\n\n        // Append the data to the appropriate sheet\n        if (experimentType === 'memory') {\n            appendMemoryData(sheet, data);\n        } else {\n            appendAttentionData(sheet, data);\n        }\n\n        // Return success response\n        return ContentService\n            .createTextOutput(JSON.stringify({\n                status: 'success',\n                experiment: experimentType,\n                sheet: sheetName\n            }))\n            .setMimeType(ContentService.MimeType.JSON);\n\n    } catch (error) {\n        debugSheet.appendRow([new Date(), 'ERROR', error.toString()]);\n        return ContentService\n            .createTextOutput(JSON.stringify({\n                status: 'error',\n                message: error.toString()\n            }))\n            .setMimeType(ContentService.MimeType.JSON);\n    }\n}\n\n/**\n * Handle GET requests (for testing the endpoint)\n */\nfunction doGet(e) {\n    return ContentService\n        .createTextOutput('Face Perception Experiments Data Collector v2.0\\n\\nSupported experiments:\\n- attention → att_acc sheet\\n- memory → memory sheet')\n        .setMimeType(ContentService.MimeType.TEXT);\n}\n\n// ============================================================================\n// EXPERIMENT 1: ATTENTION/ACCURACY\n// ============================================================================\n\n/**\n * Add headers for the Attention/Accuracy sheet\n */\nfunction addAttentionHeaders(sheet) {\n    const headers = [\n        'timestamp',\n        'participant_id',\n        // Demographics\n        'demo_age',\n        'demo_gender',\n        'demo_race',\n        'demo_education',\n        // Zoom tracking\n        'zoom_check_bypassed',\n        'zoom_check_attempts',\n        'approved_dpr',\n        'zoom_changes_count',\n        'zoom_changes',\n        'terminated_due_to_zoom',\n        // Trial data\n        'trial_number',\n        'image_name',\n        'true_race',\n        'true_gender',\n        'size_condition',\n        'smile_condition',\n        'question_order',\n        'race_options_order',\n        'smile_options_order',\n        'race_response',\n        'race_rt',\n        'race_correct',\n        'smile_response',\n        'smile_rt',\n        'smile_correct',\n        'is_practice'\n    ];\n\n    sheet.getRange(1, 1, 1, headers.length).setValues([headers]);\n    sheet.getRange(1, 1, 1, headers.length).setFontWeight('bold');\n    sheet.setFrozenRows(1);\n}\n\n/**\n * Append data from Experiment 1\n */\nfunction appendAttentionData(sheet, data) {\n    const timestamp = data.timestamp || new Date().toISOString();\n    const participantId = data.participant_id || '';\n    const demographics = data.demographics || {};\n    const zoomTracking = data.zoom_tracking || {};\n    const trials = data.trials || [];\n\n    trials.forEach(trial =&gt; {\n        const row = [\n            timestamp,\n            participantId,\n            // Demographics\n            demographics.age || '',\n            demographics.gender || '',\n            demographics.race || '',\n            demographics.education || '',\n            // Zoom tracking\n            zoomTracking.zoom_check_bypassed || false,\n            zoomTracking.zoom_check_attempts || 0,\n            zoomTracking.approved_dpr || '',\n            zoomTracking.zoom_changes_count || 0,\n            zoomTracking.zoom_changes ? JSON.stringify(zoomTracking.zoom_changes) : '',\n            zoomTracking.terminated_due_to_zoom || false,\n            // Trial data\n            trial.trial_number,\n            trial.image_name || '',\n            trial.true_race,\n            trial.true_gender,\n            trial.size_condition,\n            trial.smile_condition,\n            trial.question_order,\n            trial.race_options_order || '',\n            trial.smile_options_order || '',\n            trial.race_response,\n            trial.race_rt,\n            trial.race_correct,\n            trial.smile_response,\n            trial.smile_rt,\n            trial.smile_correct,\n            trial.is_practice\n        ];\n        sheet.appendRow(row);\n    });\n}\n\n// ============================================================================\n// EXPERIMENT 2: MEMORY\n// ============================================================================\n\n/**\n * Add headers for the memory sheet\n */\nfunction addMemoryHeaders(sheet) {\n    const headers = [\n        'timestamp',\n        'participant_id',\n        // Demographics\n        'demo_age',\n        'demo_gender',\n        'demo_race',\n        'demo_education',\n        // Zoom tracking\n        'zoom_check_bypassed',\n        'zoom_check_attempts',\n        'approved_dpr',\n        'zoom_changes_count',\n        'zoom_changes',\n        'terminated_due_to_zoom',\n        // Round info\n        'round_number',\n        'size_condition',\n        'question_type',\n        'is_practice',\n        'grid_order',\n        'input_order',\n        'response_order',\n        // Actual composition (ground truth)\n        'actual_asian',\n        'actual_black',\n        'actual_hispanic',\n        'actual_white',\n        'actual_smiling',\n        'actual_not_smiling',\n        // Race question responses\n        'asian_response',\n        'black_response',\n        'hispanic_response',\n        'white_response',\n        'asian_error',\n        'black_error',\n        'hispanic_error',\n        'white_error',\n        'asian_correct',\n        'black_correct',\n        'hispanic_correct',\n        'white_correct',\n        // Smile question responses\n        'smiling_response',\n        'not_smiling_response',\n        'smiling_error',\n        'not_smiling_error',\n        'smiling_correct',\n        'not_smiling_correct',\n        // Timing\n        'response_rt'\n    ];\n\n    sheet.getRange(1, 1, 1, headers.length).setValues([headers]);\n    sheet.getRange(1, 1, 1, headers.length).setFontWeight('bold');\n    sheet.setFrozenRows(1);\n}\n\n/**\n * Append data from Experiment 2\n */\nfunction appendMemoryData(sheet, data) {\n    const timestamp = data.timestamp || new Date().toISOString();\n    const participantId = data.participant_id || '';\n    const demographics = data.demographics || {};\n    const zoomTracking = data.zoom_tracking || {};\n    const rounds = data.rounds || [];\n\n    rounds.forEach(round =&gt; {\n        const row = [\n            timestamp,\n            participantId,\n            // Demographics\n            demographics.age || '',\n            demographics.gender || '',\n            demographics.race || '',\n            demographics.education || '',\n            // Zoom tracking\n            zoomTracking.zoom_check_bypassed || false,\n            zoomTracking.zoom_check_attempts || 0,\n            zoomTracking.approved_dpr || '',\n            zoomTracking.zoom_changes_count || 0,\n            zoomTracking.zoom_changes ? JSON.stringify(zoomTracking.zoom_changes) : '',\n            zoomTracking.terminated_due_to_zoom || false,\n            // Round info\n            round.round_number,\n            round.size_condition,\n            round.question_type,\n            round.is_practice,\n            round.grid_order || '',\n            round.input_order || '',\n            round.response_order || '',\n            // Actual composition\n            round.actual_asian,\n            round.actual_black,\n            round.actual_hispanic,\n            round.actual_white,\n            round.actual_smiling,\n            round.actual_not_smiling,\n            // Race responses (empty if smile question)\n            round.asian_response !== undefined ? round.asian_response : '',\n            round.black_response !== undefined ? round.black_response : '',\n            round.hispanic_response !== undefined ? round.hispanic_response : '',\n            round.white_response !== undefined ? round.white_response : '',\n            round.asian_error !== undefined ? round.asian_error : '',\n            round.black_error !== undefined ? round.black_error : '',\n            round.hispanic_error !== undefined ? round.hispanic_error : '',\n            round.white_error !== undefined ? round.white_error : '',\n            round.asian_correct !== undefined ? round.asian_correct : '',\n            round.black_correct !== undefined ? round.black_correct : '',\n            round.hispanic_correct !== undefined ? round.hispanic_correct : '',\n            round.white_correct !== undefined ? round.white_correct : '',\n            // Smile responses (empty if race question)\n            round.smiling_response !== undefined ? round.smiling_response : '',\n            round.not_smiling_response !== undefined ? round.not_smiling_response : '',\n            round.smiling_error !== undefined ? round.smiling_error : '',\n            round.not_smiling_error !== undefined ? round.not_smiling_error : '',\n            round.smiling_correct !== undefined ? round.smiling_correct : '',\n            round.not_smiling_correct !== undefined ? round.not_smiling_correct : '',\n            // Timing\n            round.response_rt\n        ];\n        sheet.appendRow(row);\n    });\n}\n\n// ============================================================================\n// TEST FUNCTIONS\n// ============================================================================\n\n/**\n * Test Experiment 1 (Attention/Accuracy)\n */\nfunction testAttentionExperiment() {\n    const testData = {\n        experiment: 'attention',\n        participant_id: 'TEST_ATT_' + new Date().getTime(),\n        timestamp: new Date().toISOString(),\n        demographics: {\n            age: '25',\n            gender: 'female',\n            education: 'bachelors'\n        },\n        trials: [\n            {\n                trial_number: 'practice',\n                image_name: 'white_female_smile_01_big',\n                true_race: 'white',\n                true_gender: 'female',\n                size_condition: 'big',\n                smile_condition: 'smile',\n                question_order: 'race_first',\n                race_options_order: 'white,black,asian,hispanic',\n                smile_options_order: 'yes,no',\n                race_response: 'white',\n                race_rt: 1500,\n                race_correct: true,\n                smile_response: 'yes',\n                smile_rt: 1200,\n                smile_correct: true,\n                is_practice: true\n            }\n        ]\n    };\n\n    const mockEvent = {\n        postData: { contents: JSON.stringify(testData) }\n    };\n\n    const result = doPost(mockEvent);\n    console.log('Attention Test Result:', result.getContent());\n}\n\n/**\n * Test Experiment 2 (Memory)\n */\nfunction testMemoryExperiment() {\n    const testData = {\n        experiment: 'memory',\n        participant_id: 'TEST_MEM_' + new Date().getTime(),\n        timestamp: new Date().toISOString(),\n        demographics: {\n            age: '28',\n            gender: 'male',\n            education: 'masters'\n        },\n        rounds: [\n            {\n                round_number: 'practice',\n                size_condition: 'big',\n                question_type: 'race',\n                is_practice: true,\n                grid_order: 'black_male_01:smile,asian_female_01:nosmile,white_male_01:smile,hispanic_female_01:nosmile,black_female_01:smile,asian_male_01:nosmile,white_female_01:smile,hispanic_male_01:nosmile',\n                input_order: 'hispanic,black,white,asian',\n                response_order: 'black,hispanic,white,asian',\n                actual_asian: 2,\n                actual_black: 2,\n                actual_hispanic: 2,\n                actual_white: 2,\n                actual_smiling: 4,\n                actual_not_smiling: 4,\n                asian_response: 2,\n                black_response: 2,\n                hispanic_response: 2,\n                white_response: 2,\n                asian_error: 0,\n                black_error: 0,\n                hispanic_error: 0,\n                white_error: 0,\n                asian_correct: true,\n                black_correct: true,\n                hispanic_correct: true,\n                white_correct: true,\n                response_rt: 15000\n            }\n        ]\n    };\n\n    const mockEvent = {\n        postData: { contents: JSON.stringify(testData) }\n    };\n\n    const result = doPost(mockEvent);\n    console.log('Memory Test Result:', result.getContent());\n}"
  },
  {
    "objectID": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html#troubleshooting",
    "href": "experiments/exp_2_memory/GOOGLE_SHEETS_SETUP.html#troubleshooting",
    "title": "Google Sheets Configuration",
    "section": "",
    "text": "Make sure you deployed a new version of the script after changes\nRun the test functions to manually create sheets\n\n\n\n\n\nGo to Apps Script → Executions to see error logs\nCheck the debug sheet for detailed request logging"
  }
]